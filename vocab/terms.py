determinants = 'Think of det(A) as the global scale knob of a square matrix A; non-zero means the transform is reversible and scales area or volume; zero means it collapses space so you cannot undo it.'
inverses = 'A⁻¹ is the undo-button for A; if A warps space and keeps volume non-zero, A⁻¹ unwarps it so A⁻¹A = I.'
matrices = 'Gridded containers for linear transforms and datasets; like a CSV of numbers that also acts as a function from inputs to outputs.'
vectors = 'Ordered lists with direction and size; your feature row, a gradient step, or a weight update all as arrows in space.'
scalars = 'Single numbers that scale or offset things; like a single gain value applied to a vector or loss.'
parameters = 'Dials the model learns; weights and biases that training moves to reduce your loss.'
coefficients = 'Specific parameters that multiply inputs; in regression they tell you how strongly a feature pushes the prediction.'
terms = 'Additive building blocks in an expression; each part like 2x or 3y that contributes to the whole.'
expressions = 'Math recipes built from variables, numbers, and ops; they evaluate to a value but are not asserted equal to something.'
equations = 'Claims that two expressions match; you solve them to find the unknowns consistent with that claim.'
eigenvalues = 'Stretch factors along special directions of a linear map; how much the map scales an eigenvector.'
eigenvectors = 'Directions that the transform does not rotate; the map only stretches or shrinks them.'
singularValues = 'Non-negative scale factors from SVD; the importance of orthogonal directions of your transform or data.'
singularVectors = 'The orthogonal direction sets from SVD; left for column space; right for row space.'
principalComponents = 'Orthogonal directions of maximum variance in your data; the axes that best summarize spread.'
principalComponentsAnalysis = 'Rotate and rank features by variance; keep the top components to compress while preserving structure.'
matrixDecomposition = 'Break a matrix into simpler pieces that are easier to analyze or compute with; LU; QR; SVD.'
matrixFactorization = 'Write a big matrix as product of smaller latent matrices; the backbone of recommender systems.'
matrixInversion = 'Compute A⁻¹ so you can solve Ax = b in one shot; expensive and often replaced by smarter solvers.'
matrixMultiplication = 'Compose transforms or mix features; each output entry is a row·column dot product.'
matrixAddition = 'Elementwise combine same-shaped matrices; like merging two aligned data grids.'
matrixSubtraction = 'Elementwise difference of same-shaped matrices; useful for residuals.'
matrixTranspose = 'Flip rows and columns; aligns dimensions for dot products and reveals symmetry.'
matrixDeterminant = 'Scalar scale factor of a square matrix; zero means non-invertible; magnitude measures volume scaling.'
matrixInverse = 'The unique matrix that undoes A when it exists; A⁻¹A = I.'
matrixEigenvalues = 'λ values where Av = λv for some non-zero v; summarize intrinsic scaling behavior of A.'
matrixEigenvectors = 'Non-zero vectors v that keep direction under A; only their length changes by λ.'

#--------------------------------------------------#

# Calculus

#--------------------------------------------------#
derivatives = 'Instantaneous rate of change; the slope you would see if you zoom to a point on a curve.'
gradients = 'Vector of partial derivatives; points toward steepest increase of a multivariate function.'
gradientDescent = 'Move parameters opposite the gradient; take steps downhill until the loss stops improving.'
gradientAscent = 'Move with the gradient to maximize an objective; mirror image of descent.'
gradientBoosting = 'Fit a model; measure residuals; fit the next weak learner to those residuals; repeat; errors shrink step by step.'
gradientBoostingMachine = 'Gradient boosting implemented with trees as base learners; strong general-purpose predictor.'
gradientBoostingTrees = 'Gradient boosting that uses decision trees for each stage; handles nonlinearity and interactions well.'
gradientBoostingForests = 'Ensembles of boosted trees; sometimes blended with randomization for robustness.'
gradientBoostingEnsembles = 'Collections of weak learners trained sequentially to correct prior mistakes using gradient signals.'
gradientBoostingModels = 'Any predictive models that realize the gradient boosting procedure; usually tree-based.'
gradientBoostingAlgorithms = 'Specific training recipes for boosting; learning rate; depth; loss; regularization choices.'
gradientBoostingTechniques = 'Practical knobs and tricks; shrinkage; subsampling; early stopping; monotonic constraints.'
gradientBoostingMethods = 'Variants of boosting; tree-based; linear; component-wise; tailored to different losses.'
gradientBoostingFrameworks = 'Software that implements boosting end to end; training; tuning; export.'
gradientBoostingLibraries = 'Code you import to use boosting; XGBoost; LightGBM; CatBoost.'
gradientBoostingPackages = 'Distributions you install that provide boosting functionality via pip or conda.'
gradientBoostingTools = 'Utilities around boosting; visualization; SHAP; feature importance; deployment helpers.'
gradientBoostingSoftware = 'All programs that train; analyze; and serve boosted models.'
gradientBoostingApplications = 'Use cases; tabular prediction in finance; healthcare; CTR; fraud; ranking.'
gradientBoostingSolutions = 'End-to-end pipelines where boosting is one component; from preprocessing to serving.'
integrals = 'Accumulated quantity over an interval; areas under curves; sums turned continuous.'
partialDerivatives = 'Derivatives with respect to one variable while holding the rest fixed; the entries of a gradient.'
multipleIntegrals = 'Integrals over higher-dimensional regions; compute mass; probability; or flux in space.'
lineIntegrals = 'Integrate along a curve; measures work done by a field along a path.'
surfaceIntegrals = 'Integrate over a surface; compute flux through a sheet in 3D.'
volumeIntegrals = 'Integrate over a 3D region to total up density-based quantities.'
doubleIntegrals = 'Integrate over an area in the plane; 2D accumulation.'
tripleIntegrals = 'Integrate over 3D volumes; 3D accumulation.'
chainRule = 'Differentiate a composition by multiplying the inner and outer rates; the engine of backprop.'
productRule = 'Differentiate a product by f′g + fg′.'
quotientRule = 'Differentiate a ratio by (f′g − fg′) / g².'
powerRule = 'Derivative of x^n is n x^(n−1); quick slope for polynomials.'
exponentialRule = 'Derivative of e^{kx} is k e^{kx}; exponentials keep their shape.'
logarithmicRule = 'Derivative of ln x is 1/x; logs turn multiplication into addition in calculus too.'
trigonometricRule = 'Derivatives of sin and cos cycle; d/dx sin x = cos x; d/dx cos x = −sin x.'
inverseTrigonometricRule = 'Derivatives of arcsin; arctan; etc.; e.g.; d/dx arcsin x = 1/√(1 − x²).'
hyperbolicRule = 'Derivatives of sinh and cosh mirror trig with sign changes; d/dx sinh x = cosh x.'
inverseHyperbolicRule = 'Derivatives of asinh; acosh; etc.; e.g.; d/dx asinh x = 1/√(x² + 1).'
derivativeOfConstant = 'Zero; constants do not change so their slope vanishes.'
derivativeOfFunction = 'The general idea of taking a function’s instantaneous rate of change; computed via the rules above.'
derivativeOfSum = 'Derivative distributes over addition; differentiate parts and add.'
derivativeOfDifference = 'Derivative distributes over subtraction; differentiate parts and subtract.'
derivativeOfProduct = 'Use the product rule for multiplied functions.'
derivativeOfQuotient = 'Use the quotient rule for divided functions.'
derivativeOfPower = 'Use the power rule for x^n; extend with chain rule for composite powers.'
derivativeOfExponential = 'Use the exponential rule; add chain rule if the exponent is not linear.'
derivativeOfLogarithmic = 'Use 1/x with chain rule for logs of functions.'
backpropagation = 'Systematic use of chain rule to push loss gradients from outputs back to every parameter efficiently.'
backpropagationAlgorithm = 'Forward pass to compute outputs; backward pass to accumulate partials with chain rule; update parameters.'
backpropagationNeuralNetwork = 'Any network trained by backprop; from MLPs to CNNs to RNNs.'
backpropagationNeuralNetworks = 'The family of neural architectures that rely on backprop for learning.'
backpropagationNeuralNetworksAlgorithms = 'Concrete backprop realizations; full; truncated; plus optimizers like SGD; Adam.'
backpropagationNeuralNetworksTechniques = 'Stabilizers and speedups; normalization; residuals; dropout; good init.'
backpropagationNeuralNetworksMethods = 'Implementation tactics for forward and backward passes; analytical or autodiff.'
backpropagationNeuralNetworksFrameworks = 'Autodiff libraries that handle backprop; PyTorch; TensorFlow; JAX; Keras.'
backpropagationNeuralNetworksLibraries = 'Reusable layers; losses; optimizers packaged for backprop training.'
backpropagationNeuralNetworksPackages = 'Installable bundles that provide backprop-ready components.'
backpropagationNeuralNetworksTools = 'Profilers; debuggers; visualizers that help train and inspect networks.'
backpropagationNeuralNetworksSoftware = 'All programs supporting neural nets trained by backprop; from notebooks to servers.'

#--------------------------------------------------#

# Optimization Basics

#--------------------------------------------------#
optimization = 'Choosing the best allowable point according to an objective; in ML this is minimizing loss.'
optimizationAlgorithms = 'Recipes that navigate the objective; gradient-based; evolutionary; heuristic.'
optimizationTechniques = 'Practical strategies to make optimization work; scaling; scheduling; constraints.'
optimizationMethods = 'Named mathematical approaches; gradient descent; Newton; coordinate descent; LP; QP.'
optimizationFrameworks = 'Software that structures modeling and solving of optimization problems.'
optimizationLibraries = 'APIs with solvers and utilities for optimization problems.'
optimizationPackages = 'Installable distributions bundling optimization functionality.'
optimizationTools = 'Companions for modeling; solving; visualizing; and validating solutions.'
optimizationSoftware = 'All applications that support building and solving optimization tasks.'
optimizationApplications = 'Where optimization delivers value; routing; scheduling; pricing; hyperparameter tuning.'
localMinima = 'Valleys that are lower than nearby points but not necessarily the lowest overall.'
globalMinima = 'The absolute bottom of the objective; best achievable loss.'
localMaxima = 'Peaks higher than neighbors but not the highest overall.'
globalMaxima = 'The absolute peak of the objective surface.'
localOptima = 'Any locally best point; minima or maxima depending on the task.'
globalOptima = 'The truly best point over the full feasible region.'
convexity = 'No dents; the line between any two points stays inside; makes life easier.'
convexFunctions = 'Single-basin objectives; any local minimum is global; gradients guide you reliably.'
convexSets = 'Feasible regions without holes or caves; linear mixes of points stay in the set.'
convexOptimization = 'Minimize a convex function over a convex set; guarantees global solutions with efficient methods.'
convexOptimizationProblems = 'Problems with convex objective and convex constraints; well-behaved and scalable.'
convexOptimizationAlgorithms = 'Interior-point; first-order; ellipsoid; tailored for convex structure.'
convexOptimizationTechniques = 'Duality; barriers; epigraph tricks; proximal steps to exploit structure.'
convexOptimizationMethods = 'Primal-dual strategies; cutting planes; projected gradients.'
convexOptimizationFrameworks = 'Modeling layers and solvers for convex programs; CVXPY; CVXOPT; others.'
convexOptimizationLibraries = 'Implementations of convex solvers and interfaces in various languages.'
convexOptimizationPackages = 'Installable convex optimization toolsets.'
convexOptimizationTools = 'Parsers; analyzers; and visualizers for convex models.'
convexOptimizationSoftware = 'All code that builds and solves convex programs.'

# statistics and probability

#--------------------------------------------------#
statistics = 'Turn data into insight with principled summaries and uncertainty handling; descriptive and inferential.'
probability = 'Math of uncertainty from 0 to 1; foundation for statistical inference.'
probabilityDistributions = 'Catalogs of how randomness spreads; each with parameters controlling shape and scale.'
probabilityDistributionFunctions = 'Formulas that assign likelihood to outcomes; PMFs for discrete; PDFs for continuous.'
probabilityDistributionFunctionsAlgorithms = 'Computational routines for sampling; fitting; and scoring distributions.'
probabilityDistributionFunctionsTechniques = 'Estimation and matching tactics; MLE; method of moments; goodness-of-fit.'
probabilityDistributionFunctionsMethods = 'Concrete analytical or numerical approaches for working with distributions.'
probabilityDistributionFunctionsFrameworks = 'Software that organizes distribution objects and operations; e.g.; SciPy stack.'
probabilityDistributionFunctionsLibraries = 'Code libraries providing ready-to-use distributions and utilities.'
probabilityDistributionFunctionsPackages = 'Installable bundles for statistical distribution work.'
probabilityDistributionFunctionsTools = 'Helpers for visualization; simulation; and diagnostics.'
probabilityDistributionFunctionsSoftware = 'All applications that support distribution modeling and analysis.'
distributions = 'Descriptions of how values appear across many trials; governed by parameters and shape.'
continuousDistributions = 'Random variables that take any real value in an interval; described by PDFs and integrals.'
discreteDistributions = 'Random variables that take countable values; described by PMFs and sums.'
normalDistribution = 'Symmetric bell curve; fully set by mean and standard deviation; central in CLT.'
binomialDistribution = 'Counts successes in n independent trials with success probability p.'
poissonDistribution = 'Counts rare events over time or space given an average rate λ.'
exponentialDistribution = 'Waiting time between Poisson events; memoryless with rate λ.'
uniformDistribution = 'All values in an interval are equally likely; flat likelihood.'
gammaDistribution = 'Flexible positive-only family for waiting-time sums; shape and rate control skew.'
betaDistribution = 'Distribution on [0;1] for probabilities; great as a Bayesian prior.'
chiSquaredDistribution = 'Sum of squared standard normals; used for variance tests and GOF.'
variance = 'Average squared deviation from the mean; measures spread with squared units.'
standardDeviation = 'Square root of variance; spread in original units; more interpretable.'
covariance = 'How two variables move together; positive if they rise and fall together.'
correlation = 'Scaled covariance between −1 and 1; direction and strength of linear association.'

#--------------------------------------------------#

# Statistics

#--------------------------------------------------#
statistics = 'The discipline of data reasoning; collect; summarize; model; infer; and decide under uncertainty.'
statisticsAlgorithms = 'Computational procedures for tasks like regression; clustering; testing.'
statisticsTechniques = 'Toolbox of approaches; time series; multivariate; non-parametrics.'
statisticsMethods = 'Parametric and non-parametric ways to estimate; test; and model.'
bayesTheorem = 'Update prior belief with evidence to get a posterior; P(H|D) ∝ P(D|H)P(H).'
bayesRule = 'Same as Bayes theorem; the algebra for rational belief updates.'
bayesEstimator = 'An estimator that minimizes expected loss under a prior and observed data.'
bayesEstimatorTheorem = 'Result showing when Bayes estimators minimize expected loss under chosen loss functions.'
bayesEstimatorRule = 'Guideline for constructing Bayes estimators given priors and loss.'
bayesEstimatorMethods = 'MAP; MMSE; and related Bayesian estimation approaches.'
bayesEstimatorFrameworks = 'Software for Bayesian modeling and estimation; Stan; PyMC; NumPyro.'
bayesEstimatorLibraries = 'APIs that expose samplers; distributions; and inference engines.'
bayesEstimatorPackages = 'Installable Bayesian toolkits and language bindings.'
bayesEstimatorTools = 'Diagnostics; samplers; and plotting utilities for Bayesian work.'
bayesEstimatorSoftware = 'All software that supports Bayesian estimation end to end.'
bayesianOptimization = 'Smart black-box optimization using a surrogate model and an acquisition rule to pick the next evaluation.'
bayesianOptimizationAlgorithms = 'Use Gaussian processes or similar surrogates plus EI; UCB; or TS to balance explore; exploit.'
bayesianOptimizationTechniques = 'Kernel choice; noise handling; batching; constraints; warm starts.'
bayesianOptimizationMethods = 'Different surrogate and acquisition combinations; GP-EI; TPE; RF-based BO.'
bayesianOptimizationFrameworks = 'Libraries for BO workflows; Optuna; BoTorch; scikit-optimize.'
bayesianOptimizationLibraries = 'Primitives for surrogates; acquisitions; and optimization loops.'
bayesianOptimizationPackages = 'Installable BO solutions with tuner integrations.'
bayesianOptimizationTools = 'Experiment managers; loggers; dashboards for BO campaigns.'
bayesianOptimizationSoftware = 'Complete stacks for running BO at scale.'
bayesianOptimizationApplications = 'Hyperparameter tuning; A/B design; expensive engineering simulations.'
bayesianOptimizationSolutions = 'Turnkey BO setups attached to training platforms and lab automation.'
samplingAndConditionalProbability = 'Draw representative data and reason about P(A|B); foundations for simulation and inference.'
samplingAndConditionalProbabilityAlgorithms = 'Monte Carlo; importance sampling; MCMC to approximate tough distributions.'
samplingAndConditionalProbabilityTechniques = 'Stratify; cluster; resample; or block to reduce variance and capture structure.'
samplingAndConditionalProbabilityMethods = 'Gibbs; Metropolis–Hastings; rejection; and conditional simulation schemes.'
samplingAndConditionalProbabilityFrameworks = 'Platforms that orchestrate samplers and conditional models.'
samplingAndConditionalProbabilityLibraries = 'Code that provides sampling kernels and utilities.'
samplingAndConditionalProbabilityPackages = 'Installable distributions for sampling workflows.'
samplingAndConditionalProbabilityTools = 'Visual; diagnostic; and calibration helpers for sampling.'
samplingAndConditionalProbabilitySoftware = 'All applications that support simulation and conditional probability analysis.'
samplingAndConditionalProbabilityApplications = 'Survey design; reliability; risk; Bayesian inference; synthetic data.'
samplingAndConditionalProbabilitySolutions = 'End-to-end sampling stacks wired into analytics or experimentation.'

#--------------------------------------------------#

# Miscellaneous
#--------------------------------------------------#


